{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b3bb11-fc06-4550-9ace-5188c3a78199",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run center_of_mass_calculation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb88468c-b1b1-49f5-9029-dab66a866109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 3phase_pm_engine_simulation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d9b64c-b0d4-4841-a384-8df3cf9d5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33633c8-bdf5-44c1-a602-5e02c80ee655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"full_df.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e26007-796b-407d-9e48-42d9cdd6fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d7e6d1-7e05-411b-bfe7-d461b09e14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :29]\n",
    "y = data[:, 29:] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15535f1-2efd-4b58-b154-7b2dca72cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a06bc5-d7c5-43f2-aa43-8ce43cb422f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(X_train, dtype=\"float32\")\n",
    "X_test = tf.constant(X_test, dtype=\"float32\")\n",
    "y_train = tf.constant(y_train, dtype=\"float32\")\n",
    "y_test = tf.constant(y_test, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89da2599-ce05-4aeb-aea9-accaf7cae123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([41783, 29])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a458d8f4-7eb8-4d97-a0f9-603b25828acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(100_000).batch(256)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605bdcfb-f97d-4468-ab3e-3c5cba3c865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hundred_relu(x):\n",
    "    return K.relu(x, max_value=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475c15a1-130a-4ab6-ae26-b6aa62080bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = [\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        ]\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation=hundred_relu)\n",
    "        \n",
    "    def call(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b39b25c7-edb8-4df3-939a-44c8c4e8bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EvaluatorNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3587a0b-2654-46bf-bb6a-a37691bca525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"evaluator_network_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_1 (Batc  multiple                 116       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  1920      \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  4160      \n",
      "                                                                 \n",
      " dense_11 (Dense)            multiple                  4160      \n",
      "                                                                 \n",
      " dense_12 (Dense)            multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,421\n",
      "Trainable params: 10,363\n",
      "Non-trainable params: 58\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "evaluator.build(input_shape=(None, 29))\n",
    "evaluator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0626a761-20ab-4fc3-be54-a74fee7f3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanAbsoluteError()\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a34e38c6-cc5e-4d47-a63b-43e994e4c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd39eaba-7dda-4278-a7b3-9fc5367a1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluator_train_step(params, perf):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = evaluator(params, training=True)\n",
    "        loss = loss_object(perf, predictions)\n",
    "    gradients = tape.gradient(loss, evaluator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, evaluator.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1371a4f-4caf-48a8-8b50-d05e8e7ebbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluator_test_step(params, perf):\n",
    "    predictions = evaluator(params, training=False)\n",
    "    t_loss = loss_object(perf, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "886e876a-57f4-44f5-ae05-d90fa1bd2b08",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 49.550174713134766, Test Loss: 44.553199768066406, \n",
      "Epoch 2, Loss: 36.43023681640625, Test Loss: 18.792016983032227, \n",
      "Epoch 3, Loss: 9.037068367004395, Test Loss: 6.459325790405273, \n",
      "Epoch 4, Loss: 6.010499000549316, Test Loss: 5.747402191162109, \n",
      "Epoch 5, Loss: 5.644608020782471, Test Loss: 5.5458784103393555, \n",
      "Epoch 6, Loss: 5.478583812713623, Test Loss: 5.405271053314209, \n",
      "Epoch 7, Loss: 5.336498737335205, Test Loss: 5.27909517288208, \n",
      "Epoch 8, Loss: 5.223412990570068, Test Loss: 5.165883541107178, \n",
      "Epoch 9, Loss: 5.0972819328308105, Test Loss: 5.064255237579346, \n",
      "Epoch 10, Loss: 5.003212928771973, Test Loss: 4.967496395111084, \n",
      "Epoch 11, Loss: 4.894778728485107, Test Loss: 4.878659725189209, \n",
      "Epoch 12, Loss: 4.810069561004639, Test Loss: 4.7971014976501465, \n",
      "Epoch 13, Loss: 4.742681503295898, Test Loss: 4.7197771072387695, \n",
      "Epoch 14, Loss: 4.653592109680176, Test Loss: 4.647558212280273, \n",
      "Epoch 15, Loss: 4.577672481536865, Test Loss: 4.581293106079102, \n",
      "Epoch 16, Loss: 4.500210762023926, Test Loss: 4.523637294769287, \n",
      "Epoch 17, Loss: 4.4424238204956055, Test Loss: 4.460170745849609, \n",
      "Epoch 18, Loss: 4.393631458282471, Test Loss: 4.40261697769165, \n",
      "Epoch 19, Loss: 4.319760799407959, Test Loss: 4.351979732513428, \n",
      "Epoch 20, Loss: 4.2613043785095215, Test Loss: 4.298598289489746, \n",
      "Epoch 21, Loss: 4.219854354858398, Test Loss: 4.256712436676025, \n",
      "Epoch 22, Loss: 4.162252426147461, Test Loss: 4.206792831420898, \n",
      "Epoch 23, Loss: 4.1159844398498535, Test Loss: 4.168194770812988, \n",
      "Epoch 24, Loss: 4.076533317565918, Test Loss: 4.124545574188232, \n",
      "Epoch 25, Loss: 4.043790817260742, Test Loss: 4.08549165725708, \n",
      "Epoch 26, Loss: 3.99467134475708, Test Loss: 4.044866561889648, \n",
      "Epoch 27, Loss: 3.959125518798828, Test Loss: 4.006042957305908, \n",
      "Epoch 28, Loss: 3.9142274856567383, Test Loss: 3.9701552391052246, \n",
      "Epoch 29, Loss: 3.8819150924682617, Test Loss: 3.93672251701355, \n",
      "Epoch 30, Loss: 3.8425984382629395, Test Loss: 3.9037559032440186, \n",
      "Epoch 31, Loss: 3.8135435581207275, Test Loss: 3.873284101486206, \n",
      "Epoch 32, Loss: 3.7768490314483643, Test Loss: 3.8504478931427, \n",
      "Epoch 33, Loss: 3.749351978302002, Test Loss: 3.8182735443115234, \n",
      "Epoch 34, Loss: 3.720824718475342, Test Loss: 3.7844245433807373, \n",
      "Epoch 35, Loss: 3.6965198516845703, Test Loss: 3.762068510055542, \n",
      "Epoch 36, Loss: 3.6712820529937744, Test Loss: 3.7329630851745605, \n",
      "Epoch 37, Loss: 3.6460719108581543, Test Loss: 3.712045907974243, \n",
      "Epoch 38, Loss: 3.6142120361328125, Test Loss: 3.6903297901153564, \n",
      "Epoch 39, Loss: 3.5994999408721924, Test Loss: 3.672274589538574, \n",
      "Epoch 40, Loss: 3.5711257457733154, Test Loss: 3.6441311836242676, \n",
      "Epoch 41, Loss: 3.5556864738464355, Test Loss: 3.6231043338775635, \n",
      "Epoch 42, Loss: 3.5360162258148193, Test Loss: 3.6090145111083984, \n",
      "Epoch 43, Loss: 3.5296287536621094, Test Loss: 3.5820116996765137, \n",
      "Epoch 44, Loss: 3.4973676204681396, Test Loss: 3.5627200603485107, \n",
      "Epoch 45, Loss: 3.4716312885284424, Test Loss: 3.5524990558624268, \n",
      "Epoch 46, Loss: 3.4662973880767822, Test Loss: 3.533646821975708, \n",
      "Epoch 47, Loss: 3.44985032081604, Test Loss: 3.514854907989502, \n",
      "Epoch 48, Loss: 3.4246091842651367, Test Loss: 3.5048980712890625, \n",
      "Epoch 49, Loss: 3.4118716716766357, Test Loss: 3.4806628227233887, \n",
      "Epoch 50, Loss: 3.395292282104492, Test Loss: 3.47564435005188, \n",
      "Epoch 51, Loss: 3.3862721920013428, Test Loss: 3.4533135890960693, \n",
      "Epoch 52, Loss: 3.3712615966796875, Test Loss: 3.4365100860595703, \n",
      "Epoch 53, Loss: 3.3552353382110596, Test Loss: 3.426179885864258, \n",
      "Epoch 54, Loss: 3.3461356163024902, Test Loss: 3.4113926887512207, \n",
      "Epoch 55, Loss: 3.3276023864746094, Test Loss: 3.4094979763031006, \n",
      "Epoch 56, Loss: 3.3179359436035156, Test Loss: 3.3843677043914795, \n",
      "Epoch 57, Loss: 3.3112313747406006, Test Loss: 3.3774726390838623, \n",
      "Epoch 58, Loss: 3.293621063232422, Test Loss: 3.3651912212371826, \n",
      "Epoch 59, Loss: 3.2841098308563232, Test Loss: 3.351682424545288, \n",
      "Epoch 60, Loss: 3.271591901779175, Test Loss: 3.3442862033843994, \n",
      "Epoch 61, Loss: 3.2633004188537598, Test Loss: 3.336907148361206, \n",
      "Epoch 62, Loss: 3.2553093433380127, Test Loss: 3.325563669204712, \n",
      "Epoch 63, Loss: 3.257819175720215, Test Loss: 3.314009428024292, \n",
      "Epoch 64, Loss: 3.233954906463623, Test Loss: 3.3052730560302734, \n",
      "Epoch 65, Loss: 3.241523265838623, Test Loss: 3.2957701683044434, \n",
      "Epoch 66, Loss: 3.228355884552002, Test Loss: 3.289921760559082, \n",
      "Epoch 67, Loss: 3.2162370681762695, Test Loss: 3.2809665203094482, \n",
      "Epoch 68, Loss: 3.2104837894439697, Test Loss: 3.2746829986572266, \n",
      "Epoch 69, Loss: 3.1995677947998047, Test Loss: 3.2659595012664795, \n",
      "Epoch 70, Loss: 3.1895053386688232, Test Loss: 3.2579708099365234, \n",
      "Epoch 71, Loss: 3.1862542629241943, Test Loss: 3.253798723220825, \n",
      "Epoch 72, Loss: 3.1836183071136475, Test Loss: 3.245321273803711, \n",
      "Epoch 73, Loss: 3.1696643829345703, Test Loss: 3.2389919757843018, \n",
      "Epoch 74, Loss: 3.1681911945343018, Test Loss: 3.232313871383667, \n",
      "Epoch 75, Loss: 3.166116952896118, Test Loss: 3.2276792526245117, \n",
      "Epoch 76, Loss: 3.1545753479003906, Test Loss: 3.2243168354034424, \n",
      "Epoch 77, Loss: 3.14955472946167, Test Loss: 3.215336799621582, \n",
      "Epoch 78, Loss: 3.1450319290161133, Test Loss: 3.2177770137786865, \n",
      "Epoch 79, Loss: 3.1424989700317383, Test Loss: 3.205533742904663, \n",
      "Epoch 80, Loss: 3.1394262313842773, Test Loss: 3.201289176940918, \n",
      "Epoch 81, Loss: 3.1394107341766357, Test Loss: 3.197361469268799, \n",
      "Epoch 82, Loss: 3.128166437149048, Test Loss: 3.1957764625549316, \n",
      "Epoch 83, Loss: 3.1263461112976074, Test Loss: 3.188633441925049, \n",
      "Epoch 84, Loss: 3.1208832263946533, Test Loss: 3.185701370239258, \n",
      "Epoch 85, Loss: 3.11428165435791, Test Loss: 3.1809194087982178, \n",
      "Epoch 86, Loss: 3.1095943450927734, Test Loss: 3.17710542678833, \n",
      "Epoch 87, Loss: 3.1191048622131348, Test Loss: 3.1724798679351807, \n",
      "Epoch 88, Loss: 3.1091463565826416, Test Loss: 3.1693384647369385, \n",
      "Epoch 89, Loss: 3.100267171859741, Test Loss: 3.16796612739563, \n",
      "Epoch 90, Loss: 3.100313186645508, Test Loss: 3.162924289703369, \n",
      "Epoch 91, Loss: 3.096628427505493, Test Loss: 3.164454460144043, \n",
      "Epoch 92, Loss: 3.0905871391296387, Test Loss: 3.164747953414917, \n",
      "Epoch 93, Loss: 3.0855677127838135, Test Loss: 3.156310558319092, \n",
      "Epoch 94, Loss: 3.083543062210083, Test Loss: 3.1505672931671143, \n",
      "Epoch 95, Loss: 3.0839807987213135, Test Loss: 3.1506526470184326, \n",
      "Epoch 96, Loss: 3.0752346515655518, Test Loss: 3.1499247550964355, \n",
      "Epoch 97, Loss: 3.07480525970459, Test Loss: 3.142728090286255, \n",
      "Epoch 98, Loss: 3.0838449001312256, Test Loss: 3.1512131690979004, \n",
      "Epoch 99, Loss: 3.0724949836730957, Test Loss: 3.1408190727233887, \n",
      "Epoch 100, Loss: 3.076972484588623, Test Loss: 3.146484375, \n",
      "Epoch 101, Loss: 3.0641958713531494, Test Loss: 3.1321024894714355, \n",
      "Epoch 102, Loss: 3.0643484592437744, Test Loss: 3.1318914890289307, \n",
      "Epoch 103, Loss: 3.068828821182251, Test Loss: 3.129439115524292, \n",
      "Epoch 104, Loss: 3.0606939792633057, Test Loss: 3.1359522342681885, \n",
      "Epoch 105, Loss: 3.058631658554077, Test Loss: 3.1286113262176514, \n",
      "Epoch 106, Loss: 3.0543699264526367, Test Loss: 3.124022960662842, \n",
      "Epoch 107, Loss: 3.0554354190826416, Test Loss: 3.1214940547943115, \n",
      "Epoch 108, Loss: 3.054285764694214, Test Loss: 3.1192233562469482, \n",
      "Epoch 109, Loss: 3.0546786785125732, Test Loss: 3.1197733879089355, \n",
      "Epoch 110, Loss: 3.0507137775421143, Test Loss: 3.122736692428589, \n",
      "Epoch 111, Loss: 3.0461056232452393, Test Loss: 3.1172990798950195, \n",
      "Epoch 112, Loss: 3.0429599285125732, Test Loss: 3.124429225921631, \n",
      "Epoch 113, Loss: 3.0493719577789307, Test Loss: 3.1157567501068115, \n",
      "Epoch 114, Loss: 3.0393407344818115, Test Loss: 3.1129212379455566, \n",
      "Epoch 115, Loss: 3.046870708465576, Test Loss: 3.120556354522705, \n",
      "Epoch 116, Loss: 3.0411691665649414, Test Loss: 3.1093201637268066, \n",
      "Epoch 117, Loss: 3.041102886199951, Test Loss: 3.1084086894989014, \n",
      "Epoch 118, Loss: 3.035428524017334, Test Loss: 3.1137382984161377, \n",
      "Epoch 119, Loss: 3.0383517742156982, Test Loss: 3.104710102081299, \n",
      "Epoch 120, Loss: 3.030768394470215, Test Loss: 3.107431650161743, \n",
      "Epoch 121, Loss: 3.033518075942993, Test Loss: 3.1018331050872803, \n",
      "Epoch 122, Loss: 3.03216290473938, Test Loss: 3.1009693145751953, \n",
      "Epoch 123, Loss: 3.029597759246826, Test Loss: 3.111740827560425, \n",
      "Epoch 124, Loss: 3.0351669788360596, Test Loss: 3.0983526706695557, \n",
      "Epoch 125, Loss: 3.0346148014068604, Test Loss: 3.1199519634246826, \n",
      "Epoch 126, Loss: 3.0280239582061768, Test Loss: 3.096345901489258, \n",
      "Epoch 127, Loss: 3.0247550010681152, Test Loss: 3.095698118209839, \n",
      "Epoch 128, Loss: 3.023923397064209, Test Loss: 3.1048693656921387, \n",
      "Epoch 129, Loss: 3.0238330364227295, Test Loss: 3.1094770431518555, \n",
      "Epoch 130, Loss: 3.0251851081848145, Test Loss: 3.0956246852874756, \n",
      "Epoch 131, Loss: 3.0228517055511475, Test Loss: 3.091867685317993, \n",
      "Epoch 132, Loss: 3.023160219192505, Test Loss: 3.0910887718200684, \n",
      "Epoch 133, Loss: 3.0250179767608643, Test Loss: 3.0915632247924805, \n",
      "Epoch 134, Loss: 3.022296190261841, Test Loss: 3.090517044067383, \n",
      "Epoch 135, Loss: 3.0117475986480713, Test Loss: 3.0927677154541016, \n",
      "Epoch 136, Loss: 3.014871835708618, Test Loss: 3.0892465114593506, \n",
      "Epoch 137, Loss: 3.015187978744507, Test Loss: 3.087991952896118, \n",
      "Epoch 138, Loss: 3.0122294425964355, Test Loss: 3.087984323501587, \n",
      "Epoch 139, Loss: 3.0190398693084717, Test Loss: 3.085256576538086, \n",
      "Epoch 140, Loss: 3.009296417236328, Test Loss: 3.0880751609802246, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params, perf \u001b[38;5;129;01min\u001b[39;00m train_ds:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mevaluator_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params, perf \u001b[38;5;129;01min\u001b[39;00m test_ds:\n\u001b[1;32m     13\u001b[0m     evaluator_test_step(params, perf)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "\n",
    "    i = 0\n",
    "    for params, perf in train_ds:\n",
    "        evaluator_train_step(params, perf)\n",
    "\n",
    "    for params, perf in test_ds:\n",
    "        evaluator_test_step(params, perf)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27df9ca1-40b4-4977-9b10-5a9e4eea7c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[48.44828]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(tf.reshape(canonical_parameters, shape=(1, 29)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb731421-97e1-492d-bce7-4c536846ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_parameters = tf.constant(\n",
    "    [25.276] * 17 + [16] * 12,\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "canonical_parameters = tf.reshape(canonical_parameters, shape=(1, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cd8a4b9-cc73-4af4-976a-23777f6d8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mult = tf.constant([45] * 17 + [8] * 12, dtype=\"float32\")\n",
    "trans_add = tf.constant([5] * 17 + [16] * 12, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69773652-4e44-4b07-86ff-9037c416cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = [\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        ]\n",
    "        self.output_layer = tf.keras.layers.Dense(29, activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        x *= trans_mult\n",
    "        x += trans_add\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1e2d2939-9177-4bba-9a33-55a69c91ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = OptimizerNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "30d5f240-7483-46bf-9b17-065a83277787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"optimizer_network_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            multiple                  3840      \n",
      "                                                                 \n",
      " dense_49 (Dense)            multiple                  16512     \n",
      "                                                                 \n",
      " dense_50 (Dense)            multiple                  8256      \n",
      "                                                                 \n",
      " dense_51 (Dense)            multiple                  2080      \n",
      "                                                                 \n",
      " dense_52 (Dense)            multiple                  957       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,645\n",
      "Trainable params: 31,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer.build(input_shape=(None, 29))\n",
    "optimizer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "18fb9f61-d614-4726-a9ef-6f60d22445d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanAbsoluteError()\n",
    "nadam = tf.keras.optimizers.Nadam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c57e466-d412-4437-abaa-c42542133580",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "82a77322-bb64-48fe-8893-a0886fab3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def optimizer_train_step(params):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = optimizer(params, training=True)\n",
    "        evaluation = evaluator(predictions, training=False) / 100\n",
    "        loss = 1 - evaluation\n",
    "    gradients = tape.gradient(loss, optimizer.trainable_variables)\n",
    "    nadam.apply_gradients(zip(gradients, optimizer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e503949a-1085-4fe4-93fc-843ee597bb45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.46284377574920654, \n",
      "Epoch 2, Loss: 0.4720079302787781, \n",
      "Epoch 3, Loss: 0.4670640230178833, \n",
      "Epoch 4, Loss: 0.46666133403778076, \n",
      "Epoch 5, Loss: 0.4637908339500427, \n",
      "Epoch 6, Loss: 0.4624338150024414, \n",
      "Epoch 7, Loss: 0.4603308439254761, \n",
      "Epoch 8, Loss: 0.4576895833015442, \n",
      "Epoch 9, Loss: 0.45405858755111694, \n",
      "Epoch 10, Loss: 0.44905245304107666, \n",
      "Epoch 11, Loss: 0.4426628351211548, \n",
      "Epoch 12, Loss: 0.4371417760848999, \n",
      "Epoch 13, Loss: 0.4330097436904907, \n",
      "Epoch 14, Loss: 0.4301784038543701, \n",
      "Epoch 15, Loss: 0.4270440340042114, \n",
      "Epoch 16, Loss: 0.42470741271972656, \n",
      "Epoch 17, Loss: 0.4217052459716797, \n",
      "Epoch 18, Loss: 0.4192119836807251, \n",
      "Epoch 19, Loss: 0.41546785831451416, \n",
      "Epoch 20, Loss: 0.4122946858406067, \n",
      "Epoch 21, Loss: 0.40922510623931885, \n",
      "Epoch 22, Loss: 0.4070524573326111, \n",
      "Epoch 23, Loss: 0.4041033387184143, \n",
      "Epoch 24, Loss: 0.4028396010398865, \n",
      "Epoch 25, Loss: 0.3998422622680664, \n",
      "Epoch 26, Loss: 0.39847850799560547, \n",
      "Epoch 27, Loss: 0.3960103988647461, \n",
      "Epoch 28, Loss: 0.39448970556259155, \n",
      "Epoch 29, Loss: 0.39155203104019165, \n",
      "Epoch 30, Loss: 0.38932907581329346, \n",
      "Epoch 31, Loss: 0.38665539026260376, \n",
      "Epoch 32, Loss: 0.3843739628791809, \n",
      "Epoch 33, Loss: 0.3812340497970581, \n",
      "Epoch 34, Loss: 0.3788635730743408, \n",
      "Epoch 35, Loss: 0.37657421827316284, \n",
      "Epoch 36, Loss: 0.3746839165687561, \n",
      "Epoch 37, Loss: 0.3720816969871521, \n",
      "Epoch 38, Loss: 0.3696526885032654, \n",
      "Epoch 39, Loss: 0.3673732280731201, \n",
      "Epoch 40, Loss: 0.36615484952926636, \n",
      "Epoch 41, Loss: 0.3637274503707886, \n",
      "Epoch 42, Loss: 0.36159467697143555, \n",
      "Epoch 43, Loss: 0.35935938358306885, \n",
      "Epoch 44, Loss: 0.35740482807159424, \n",
      "Epoch 45, Loss: 0.3554837703704834, \n",
      "Epoch 46, Loss: 0.3519158363342285, \n",
      "Epoch 47, Loss: 0.34991252422332764, \n",
      "Epoch 48, Loss: 0.34651243686676025, \n",
      "Epoch 49, Loss: 0.3447531461715698, \n",
      "Epoch 50, Loss: 0.3422201871871948, \n",
      "Epoch 51, Loss: 0.3405774235725403, \n",
      "Epoch 52, Loss: 0.3388003706932068, \n",
      "Epoch 53, Loss: 0.3372156023979187, \n",
      "Epoch 54, Loss: 0.3359054923057556, \n",
      "Epoch 55, Loss: 0.33473384380340576, \n",
      "Epoch 56, Loss: 0.33294355869293213, \n",
      "Epoch 57, Loss: 0.33104974031448364, \n",
      "Epoch 58, Loss: 0.32904160022735596, \n",
      "Epoch 59, Loss: 0.3268159031867981, \n",
      "Epoch 60, Loss: 0.3241521716117859, \n",
      "Epoch 61, Loss: 0.3218337893486023, \n",
      "Epoch 62, Loss: 0.3192020058631897, \n",
      "Epoch 63, Loss: 0.31730037927627563, \n",
      "Epoch 64, Loss: 0.31544607877731323, \n",
      "Epoch 65, Loss: 0.3136741518974304, \n",
      "Epoch 66, Loss: 0.3123558759689331, \n",
      "Epoch 67, Loss: 0.310993492603302, \n",
      "Epoch 68, Loss: 0.30988746881484985, \n",
      "Epoch 69, Loss: 0.3087735176086426, \n",
      "Epoch 70, Loss: 0.3078237771987915, \n",
      "Epoch 71, Loss: 0.3069591522216797, \n",
      "Epoch 72, Loss: 0.30614709854125977, \n",
      "Epoch 73, Loss: 0.3054579496383667, \n",
      "Epoch 74, Loss: 0.30477404594421387, \n",
      "Epoch 75, Loss: 0.3041911721229553, \n",
      "Epoch 76, Loss: 0.30363601446151733, \n",
      "Epoch 77, Loss: 0.30317413806915283, \n",
      "Epoch 78, Loss: 0.3027001619338989, \n",
      "Epoch 79, Loss: 0.302229642868042, \n",
      "Epoch 80, Loss: 0.3018531799316406, \n",
      "Epoch 81, Loss: 0.30148667097091675, \n",
      "Epoch 82, Loss: 0.3011065721511841, \n",
      "Epoch 83, Loss: 0.30079013109207153, \n",
      "Epoch 84, Loss: 0.3004882335662842, \n",
      "Epoch 85, Loss: 0.3001863360404968, \n",
      "Epoch 86, Loss: 0.2999224066734314, \n",
      "Epoch 87, Loss: 0.2996901869773865, \n",
      "Epoch 88, Loss: 0.29943978786468506, \n",
      "Epoch 89, Loss: 0.2992282509803772, \n",
      "Epoch 90, Loss: 0.2990114092826843, \n",
      "Epoch 91, Loss: 0.298800528049469, \n",
      "Epoch 92, Loss: 0.2986277937889099, \n",
      "Epoch 93, Loss: 0.29845893383026123, \n",
      "Epoch 94, Loss: 0.2983074188232422, \n",
      "Epoch 95, Loss: 0.2981370687484741, \n",
      "Epoch 96, Loss: 0.29798221588134766, \n",
      "Epoch 97, Loss: 0.29784268140792847, \n",
      "Epoch 98, Loss: 0.29770487546920776, \n",
      "Epoch 99, Loss: 0.2975676655769348, \n",
      "Epoch 100, Loss: 0.2974720001220703, \n",
      "Epoch 101, Loss: 0.29733753204345703, \n",
      "Epoch 102, Loss: 0.2972288131713867, \n",
      "Epoch 103, Loss: 0.2971190810203552, \n",
      "Epoch 104, Loss: 0.2970096468925476, \n",
      "Epoch 105, Loss: 0.2969195246696472, \n",
      "Epoch 106, Loss: 0.2968258857727051, \n",
      "Epoch 107, Loss: 0.296741783618927, \n",
      "Epoch 108, Loss: 0.29665035009384155, \n",
      "Epoch 109, Loss: 0.2965611219406128, \n",
      "Epoch 110, Loss: 0.2964836359024048, \n",
      "Epoch 111, Loss: 0.29640984535217285, \n",
      "Epoch 112, Loss: 0.29633861780166626, \n",
      "Epoch 113, Loss: 0.2962667942047119, \n",
      "Epoch 114, Loss: 0.29619401693344116, \n",
      "Epoch 115, Loss: 0.296123206615448, \n",
      "Epoch 116, Loss: 0.2960638403892517, \n",
      "Epoch 117, Loss: 0.2959975600242615, \n",
      "Epoch 118, Loss: 0.29593658447265625, \n",
      "Epoch 119, Loss: 0.295889675617218, \n",
      "Epoch 120, Loss: 0.29582709074020386, \n",
      "Epoch 121, Loss: 0.29577475786209106, \n",
      "Epoch 122, Loss: 0.29572105407714844, \n",
      "Epoch 123, Loss: 0.2956671118736267, \n",
      "Epoch 124, Loss: 0.2956209182739258, \n",
      "Epoch 125, Loss: 0.2955792546272278, \n",
      "Epoch 126, Loss: 0.29552948474884033, \n",
      "Epoch 127, Loss: 0.2954856753349304, \n",
      "Epoch 128, Loss: 0.29543983936309814, \n",
      "Epoch 129, Loss: 0.29539626836776733, \n",
      "Epoch 130, Loss: 0.2953607439994812, \n",
      "Epoch 131, Loss: 0.2953200936317444, \n",
      "Epoch 132, Loss: 0.29528141021728516, \n",
      "Epoch 133, Loss: 0.29524439573287964, \n",
      "Epoch 134, Loss: 0.2952059507369995, \n",
      "Epoch 135, Loss: 0.29517078399658203, \n",
      "Epoch 136, Loss: 0.295138418674469, \n",
      "Epoch 137, Loss: 0.2951059341430664, \n",
      "Epoch 138, Loss: 0.2950740456581116, \n",
      "Epoch 139, Loss: 0.2950410842895508, \n",
      "Epoch 140, Loss: 0.2950085401535034, \n",
      "Epoch 141, Loss: 0.2949798107147217, \n",
      "Epoch 142, Loss: 0.29495173692703247, \n",
      "Epoch 143, Loss: 0.29492461681365967, \n",
      "Epoch 144, Loss: 0.29489678144454956, \n",
      "Epoch 145, Loss: 0.2948678731918335, \n",
      "Epoch 146, Loss: 0.29484063386917114, \n",
      "Epoch 147, Loss: 0.2948172092437744, \n",
      "Epoch 148, Loss: 0.2947911024093628, \n",
      "Epoch 149, Loss: 0.29476696252822876, \n",
      "Epoch 150, Loss: 0.2947433590888977, \n",
      "Epoch 151, Loss: 0.29471850395202637, \n",
      "Epoch 152, Loss: 0.29469597339630127, \n",
      "Epoch 153, Loss: 0.2946740984916687, \n",
      "Epoch 154, Loss: 0.29465198516845703, \n",
      "Epoch 155, Loss: 0.2946318984031677, \n",
      "Epoch 156, Loss: 0.29461097717285156, \n",
      "Epoch 157, Loss: 0.29458940029144287, \n",
      "Epoch 158, Loss: 0.2945701479911804, \n",
      "Epoch 159, Loss: 0.29455047845840454, \n",
      "Epoch 160, Loss: 0.29453110694885254, \n",
      "Epoch 161, Loss: 0.2945147752761841, \n",
      "Epoch 162, Loss: 0.2944953441619873, \n",
      "Epoch 163, Loss: 0.2944765090942383, \n",
      "Epoch 164, Loss: 0.2944601774215698, \n",
      "Epoch 165, Loss: 0.29444247484207153, \n",
      "Epoch 166, Loss: 0.2944272756576538, \n",
      "Epoch 167, Loss: 0.29441046714782715, \n",
      "Epoch 168, Loss: 0.29439377784729004, \n",
      "Epoch 169, Loss: 0.29437732696533203, \n",
      "Epoch 170, Loss: 0.29436302185058594, \n",
      "Epoch 171, Loss: 0.29434734582901, \n",
      "Epoch 172, Loss: 0.2943320870399475, \n",
      "Epoch 173, Loss: 0.2943207621574402, \n",
      "Epoch 174, Loss: 0.2943045496940613, \n",
      "Epoch 175, Loss: 0.2942906618118286, \n",
      "Epoch 176, Loss: 0.2942771911621094, \n",
      "Epoch 177, Loss: 0.2942628860473633, \n",
      "Epoch 178, Loss: 0.29425013065338135, \n",
      "Epoch 179, Loss: 0.2942392826080322, \n",
      "Epoch 180, Loss: 0.2942253351211548, \n",
      "Epoch 181, Loss: 0.29421311616897583, \n",
      "Epoch 182, Loss: 0.2942003011703491, \n",
      "Epoch 183, Loss: 0.2941877841949463, \n",
      "Epoch 184, Loss: 0.2941766381263733, \n",
      "Epoch 185, Loss: 0.2941664457321167, \n",
      "Epoch 186, Loss: 0.2941548228263855, \n",
      "Epoch 187, Loss: 0.2941431403160095, \n",
      "Epoch 188, Loss: 0.29413193464279175, \n",
      "Epoch 189, Loss: 0.2941206693649292, \n",
      "Epoch 190, Loss: 0.2941110134124756, \n",
      "Epoch 191, Loss: 0.2941020131111145, \n",
      "Epoch 192, Loss: 0.29409122467041016, \n",
      "Epoch 193, Loss: 0.29408061504364014, \n",
      "Epoch 194, Loss: 0.294070303440094, \n",
      "Epoch 195, Loss: 0.2940608859062195, \n",
      "Epoch 196, Loss: 0.29405176639556885, \n",
      "Epoch 197, Loss: 0.29404276609420776, \n",
      "Epoch 198, Loss: 0.29403358697891235, \n",
      "Epoch 199, Loss: 0.29402416944503784, \n",
      "Epoch 200, Loss: 0.29401493072509766, \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "possible_densities = tf.random.uniform((1, 17), minval=5, maxval=50, dtype=\"float32\", seed=42)\n",
    "params = tf.random.uniform((1, 12), minval=16, maxval=24, dtype=\"float32\", seed=42)\n",
    "params = tf.concat([possible_densities, params], axis=1)\n",
    "\n",
    "params = tf.reshape(params, shape=(1, 29))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    params = optimizer_train_step(params)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d315d2ed-405b-4cca-b1ce-8affb2608daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 29), dtype=float32, numpy=\n",
       "array([[49.94231  , 49.999985 ,  5.005673 ,  5.1826606,  5.000003 ,\n",
       "        49.999695 , 49.999737 ,  5.0000687,  5.125733 , 50.       ,\n",
       "         5.188297 ,  5.       ,  5.180157 , 49.998127 ,  5.       ,\n",
       "         5.       ,  5.00009  , 23.999966 , 24.       , 24.       ,\n",
       "        24.       , 23.984385 , 23.998886 , 16.       , 16.000551 ,\n",
       "        23.999825 , 23.998434 , 16.002691 , 23.98083  ]], dtype=float32)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "33ecfcef-8a9e-44dc-ac93-c45883be00e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[-0.00350823],\n",
       "        [ 0.06379493]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.886479>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_center_of_mass(tf.reshape(params, (29,)), show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b4336892-d8af-4e7b-aabf-7a110ae49063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.7826712>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.linspace(0, 120, 1000)\n",
    "evaluate_efficiency(tf.reshape(params, (29,)), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f114353-0282-449e-9cd9-7b2d0f68961a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.51934665>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_efficiency(tf.reshape(canonical_parameters, (29,)), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0aa71fee-3723-413c-bc6f-1ccb0803cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.concat([tf.reshape(params, (29,)), tf.constant([0.7826712])], axis=0).numpy()\n",
    "d = tf.reshape(d, shape=(30, 1))\n",
    "bdf = pd.DataFrame(d)\n",
    "bdf.to_csv(\"best_params.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4583ba-77f7-469d-8a5d-ba4fba02f2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
